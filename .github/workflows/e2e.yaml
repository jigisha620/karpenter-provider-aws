name: E2E
on:
  workflow_dispatch:
    inputs:
      git_ref:
        type: string
      region:
        type: choice
        options:
          - "us-east-1"
          - "us-east-2"
          - "us-west-2"
          - "eu-west-1"
        default: "us-east-2"
      suite:
        type: choice
        required: true
        options:
          - Integration
          - NodeClaim
          - Consolidation
          - Interruption
          - Drift
          - Expiration
          - Chaos
          - IPv6
          - Scale
          - PrivateCluster
          - LocalZone
      k8s_version:
        type: choice
        options:
          - "1.23"
          - "1.24"
          - "1.25"
          - "1.26"
          - "1.27"
          - "1.28"
          - "1.29"
        default: "1.29"
      cluster_name:
        type: string
      cleanup:
        type: boolean
        required: true
        default: true
      enable_metrics:
        type: boolean
        default: false
  workflow_call:
    inputs:
      git_ref:
        type: string
      region:
        type: string
        default: "us-east-2"
      suite:
        type: string
        required: true
      k8s_version:
        type: string
        default: "1.29"
      enable_metrics:
        type: boolean
        default: false
      cleanup:
        type: boolean
        required: true
      workflow_trigger:
        type: string
      cluster_name:
        type: string
        description: If cluster_name is empty, a new cluster will be created. Otherwise, tests will run on an existing cluster
    secrets:
      SLACK_WEBHOOK_URL:
        required: false
      SLACK_WEBHOOK_SOAK_URL:
        required: false
jobs:
  run-suite:
    permissions:
      id-token: write # aws-actions/configure-aws-credentials@v4.0.1
      statuses: write # ./.github/actions/commit-status/start
    name: suite-${{ inputs.suite }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@9bb56186c3b09b4f86b1c65136769dd318469633 # v4.1.2
        with:
          ref: ${{ inputs.git_ref }}
      - if: always() && github.event_name == 'workflow_run'
        uses: ./.github/actions/commit-status/start
        with:
          name: ${{ github.workflow }} (${{ inputs.k8s_version }}) / e2e (${{ inputs.suite }})
          git_ref: ${{ inputs.git_ref }}
      - uses: ./.github/actions/install-deps
      - name: configure aws credentials
        uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502 # v4.0.2
        with:
          role-to-assume: arn:aws:iam::${{ vars.CI_ACCOUNT_ID }}:role/${{ vars.CI_ROLE_NAME }}
          aws-region: ${{ inputs.region }}
          role-duration-seconds: 21600
      - name: add jitter on cluster setup
        run: |
          # Creating jitter so that we can stagger cluster creation to avoid throttling
          sleep $(( RANDOM % 300 + 1 ))
      - id: generate-cluster-name
        name: generate cluster name
        env:
          SUITE: ${{ inputs.suite }}
          CLUSTER_NAME: ${{ inputs.cluster_name }}
          WORKFLOW_TRIGGER: ${{ inputs.workflow_trigger }}
        run: |
          if [[ "$CLUSTER_NAME" == '' ]]; then
            if [[ "$WORKFLOW_TRIGGER" == 'soak' ]]; then
              CLUSTER_NAME=$(echo "soak-periodic-$RANDOM$RANDOM" | awk '{print tolower($0)}' | tr / -)
            else
              CLUSTER_NAME=$(echo "$SUITE-$RANDOM$RANDOM" | awk '{print tolower($0)}' | tr / -)
            fi
          fi
          echo "Using cluster name \"$CLUSTER_NAME\""
          echo CLUSTER_NAME="$CLUSTER_NAME" >> "$GITHUB_OUTPUT"
      - name: generate codeBuild region variable
        env:
          region: ${{ inputs.region }}
        run: |
          # Set codeBuild Region to get the value of codebuild security group and VPC needed to configure private cluster
          codebuild_region="${region//-/_}"
          echo CODEBUILD_REGION="$codebuild_region" >> "$GITHUB_ENV"
      - name: setup eks cluster '${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}'
        if: inputs.cluster_name == ''
        uses: ./.github/actions/e2e/setup-cluster
        with:
          account_id: ${{ vars.CI_ACCOUNT_ID }}
          role: ${{ vars.CI_ROLE_NAME }}
          region: ${{ inputs.region }}
          cluster_name: ${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}
          k8s_version: ${{ inputs.k8s_version }}
          eksctl_version: v0.169.0
          ip_family: ${{ contains(inputs.suite, 'IPv6') && 'IPv6' || 'IPv4' }} # Set the value to IPv6 if IPv6 suite, else IPv4
          private_cluster: ${{ inputs.suite == 'PrivateCluster' }}
          git_ref: ${{ inputs.git_ref }}
          ecr_account_id: ${{ vars.SNAPSHOT_ACCOUNT_ID }}
          ecr_region: ${{ vars.SNAPSHOT_REGION }}
          prometheus_workspace_id: ${{ vars.WORKSPACE_ID }}
          prometheus_region: ${{ vars.PROMETHEUS_REGION }}
          enable_local_zones: ${{ inputs.suite == 'LocalZone' }}
          cleanup: ${{ inputs.cleanup }}
          codebuild_sg: ${{ vars[format('{0}_CODEBUILD_SG', env.CODEBUILD_REGION)] }}
          codebuild_vpc: ${{ vars[format('{0}_CODEBUILD_VPC', env.CODEBUILD_REGION)] }}
          codebuild_role: ${{ vars[format('{0}_CODEBUILD_ROLE', env.CODEBUILD_REGION)] }}
      - name: run tests for private cluster
        if: ${{ inputs.suite == 'PrivateCluster' }}
        env:
          # If we are performing the PrivateCluster test suite, then we should just run the 'Integration' test suite
          SUITE: "Integration"
          CLUSTER_NAME: ${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}
          INTERRUPTION_QUEUE: ${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}
          VERSION: v3.12.3 # Pinned to this version since v3.13.0 has issues with anonymous pulls: https://github.com/helm/helm/issues/12423
          PROMETHEUS_REGION: ${{ vars.PROMETHEUS_REGION }}
          WORKSPACE_ID: ${{ vars.WORKSPACE_ID }}
          ACCOUNT_ID: ${{ vars.CI_ACCOUNT_ID }}
          K8S_VERSION: ${{ inputs.k8s_version }}
          ECR_ACCOUNT_ID: ${{ vars.SNAPSHOT_ACCOUNT_ID }}
          ECR_REGION: ${{ vars.SNAPSHOT_REGION }}
          PRIVATE_CLUSTER: ${{ inputs.suite == 'PrivateCluster' }}
          ENABLE_METRICS: ${{ inputs.enable_metrics }}
          METRICS_REGION: ${{ vars.TIMESTREAM_REGION }}
          VPC_PEERING_CONNECTION_ID:  ${{ env.VPC_PEERING_CONNECTION_ID }}
          NODE_ROLE: ${{ env.NODE_ROLE }}
          SG_CB: ${{ vars[format('{0}_CODEBUILD_SG', env.CODEBUILD_REGION)] }}
          VPC_CB: ${{ vars[format('{0}_CODEBUILD_VPC', env.CODEBUILD_REGION)] }}
          CLUSTER_VPC_ID: ${{ env.CLUSTER_VPC_ID }}
          EKS_CLUSTER_SG: ${{ env.EKS_CLUSTER_SG }}
        uses: aws-actions/aws-codebuild-run-build@bafa4d8b0d8802b5adf3a54861f530792d2e4f24 #v1.0.15
        with:
          project-name: E2EPrivateClusterCodeBuildProject-us-west-2
          buildspec-override: |
            version: 0.2
            phases:
              install:
                commands:
                  # Make sure goenv is up to date
                  - cd $HOME/.goenv && git pull --ff-only && cd -
                  # Install Go 1.22
                  - goenv install 1.22 && goenv global 1.22
              build:
                commands:
                  - aws eks update-kubeconfig --name $CLUSTER_NAME
                  - aws ecr get-login-password --region $ECR_REGION | docker login --username AWS --password-stdin $ECR_ACCOUNT_ID.dkr.ecr.$ECR_REGION.amazonaws.com
                  - chmod +x ./hack/scripts/noderole_bootstrap_permission.sh && ./hack/scripts/noderole_bootstrap_permission.sh
                  - chmod +x ./hack/scripts/install_helm.sh && ./hack/scripts/install_helm.sh
                  - helm plugin install https://github.com/databus23/helm-diff || true
                  - helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
                  - helm pull prometheus-community/kube-prometheus-stack
                  - kubectl create ns prometheus || true
                  - kubectl label ns prometheus scrape=enabled --overwrite=true
                  - chmod +x ./hack/scripts/install_prometheus.sh && ./hack/scripts/install_prometheus.sh
                  - kubectl label ns kube-system scrape=enabled --overwrite=true
                  - kubectl label ns kube-system pod-security.kubernetes.io/warn=restricted --overwrite=true
                  - chmod +x ./hack/scripts/install_karpenter.sh && ./hack/scripts/install_karpenter.sh
                  - chmod +x ./hack/scripts/diff_karpenter.sh && ./hack/scripts/diff_karpenter.sh
                  - kubectl delete nodepool --all
                  - kubectl delete ec2nodeclass --all
                  - kubectl delete deployment --all
              post_build:
                commands:
                  # Describe karpenter pods
                  - kubectl describe pods -n kube-system -l app.kubernetes.io/name=karpenter
                  # Describe nodes
                  - kubectl describe nodes
                  - chmod +x ./hack/scripts/clean_private_cluster.sh && ./hack/scripts/clean_private_cluster.sh
          env-vars-for-codebuild: |
            SUITE,
            CLUSTER_NAME,
            INTERRUPTION_QUEUE,
            VERSION,
            PROMETHEUS_REGION,
            WORKSPACE_ID,
            ACCOUNT_ID,
            K8S_VERSION,
            ECR_ACCOUNT_ID,
            ECR_REGION,
            PRIVATE_CLUSTER,
            ENABLE_METRICS,
            METRICS_REGION,
            VPC_PEERING_CONNECTION_ID,
            NODE_ROLE,
            SG_CB,
            VPC_CB,
            CLUSTER_VPC_ID,
            EKS_CLUSTER_SG
      - name: run the ${{ inputs.suite }} test suite
        if: ${{ inputs.suite != 'PrivateCluster' }}
        env:
          SUITE: ${{ inputs.suite }}
          ENABLE_METRICS: ${{ inputs.enable_metrics }}
        run: |
          aws eks update-kubeconfig --name ${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}
          # Clean up the cluster before running all tests
          kubectl delete nodepool --all
          kubectl delete ec2nodeclass --all
          kubectl delete deployment --all

          TEST_SUITE="$SUITE" ENABLE_METRICS=$ENABLE_METRICS METRICS_REGION=${{ vars.TIMESTREAM_REGION }} GIT_REF="$(git rev-parse HEAD)" \
            CLUSTER_NAME="${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}" CLUSTER_ENDPOINT="$(aws eks describe-cluster --name ${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }} --query "cluster.endpoint" --output text)" \
            INTERRUPTION_QUEUE="${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}" make e2etests
      - name: notify slack of success or failure
        uses: ./.github/actions/e2e/slack/notify
        if: (success() || failure()) && github.event_name != 'workflow_run' && inputs.workflow_trigger != 'versionCompatibility'
        with:
          cluster_name: ${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}
          url: ${{ inputs.workflow_trigger == 'soak' && secrets.SLACK_WEBHOOK_SOAK_URL || secrets.SLACK_WEBHOOK_URL }}
          suite: ${{ inputs.workflow_trigger == 'soak' && 'soak' || inputs.suite }}
          git_ref: ${{ inputs.git_ref }}
      - name: dump logs on failure
        uses: ./.github/actions/e2e/dump-logs
        if: (failure() || cancelled()) && inputs.suite != 'PrivateCluster'
        with:
          account_id: ${{ vars.CI_ACCOUNT_ID }}
          role: ${{ vars.CI_ROLE_NAME }}
          region: ${{ inputs.region }}
          cluster_name: ${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}
      - name: cleanup karpenter and cluster '${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}' resources
        uses: ./.github/actions/e2e/cleanup
        if: always() && inputs.cleanup
        with:
          account_id: ${{ vars.CI_ACCOUNT_ID }}
          role: ${{ vars.CI_ROLE_NAME }}
          region: ${{ inputs.region }}
          cluster_name: ${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}
          git_ref: ${{ inputs.git_ref }}
          eksctl_version: v0.169.0
          private_cluster: ${{ inputs.suite == 'PrivateCluster' }}
      - if: always() && github.event_name == 'workflow_run'
        uses: ./.github/actions/commit-status/end
        with:
          name: ${{ github.workflow }} (${{ inputs.k8s_version }}) / e2e (${{ inputs.suite }})
          git_ref: ${{ inputs.git_ref }}
