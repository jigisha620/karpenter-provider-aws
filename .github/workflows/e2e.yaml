name: E2E
on:
  workflow_dispatch:
    inputs:
      git_ref:
        type: string
      region:
        type: choice
        options:
          - "us-east-1"
          - "us-east-2"
          - "us-west-2"
          - "eu-west-1"
        default: "us-east-2"
      suite:
        type: choice
        required: true
        options:
          - Integration
          - NodeClaim
          - Consolidation
          - Interruption
          - Drift
          - Expiration
          - Chaos
          - IPv6
          - Scale
          - PrivateCluster
          - LocalZone
      k8s_version:
        type: choice
        options:
          - "1.23"
          - "1.24"
          - "1.25"
          - "1.26"
          - "1.27"
          - "1.28"
          - "1.29"
        default: "1.29"
      cluster_name:
        type: string
      cleanup:
        type: boolean
        required: true
        default: true
      enable_metrics:
        type: boolean
        default: false
  workflow_call:
    inputs:
      git_ref:
        type: string
      region:
        type: string
        default: "us-east-2"
      suite:
        type: string
        required: true
      k8s_version:
        type: string
        default: "1.29"
      enable_metrics:
        type: boolean
        default: false
      cleanup:
        type: boolean
        required: true
      workflow_trigger:
        type: string
      cluster_name:
        type: string
        description: If cluster_name is empty, a new cluster will be created. Otherwise, tests will run on an existing cluster
    secrets:
      SLACK_WEBHOOK_URL:
        required: false
      SLACK_WEBHOOK_SOAK_URL:
        required: false
jobs:
  run-suite:
    permissions:
      id-token: write # aws-actions/configure-aws-credentials@v4.0.1
      statuses: write # ./.github/actions/commit-status/start
    name: suite-${{ inputs.suite }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
        with:
          ref: ${{ inputs.git_ref }}
      - if: always() && github.event_name == 'workflow_run'
        uses: ./.github/actions/commit-status/start
        with:
          name: ${{ github.workflow }} (${{ inputs.k8s_version }}) / e2e (${{ inputs.suite }})
          git_ref: ${{ inputs.git_ref }}
      - uses: ./.github/actions/install-deps
      - name: configure aws credentials
        uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502 # v4.0.2
        with:
          role-to-assume: arn:aws:iam::${{ vars.CI_ACCOUNT_ID }}:role/${{ vars.CI_ROLE_NAME }}
          aws-region: ${{ inputs.region }}
          role-duration-seconds: 21600
      - name: add jitter on cluster setup
        run: |
          # Creating jitter so that we can stagger cluster creation to avoid throttling
          sleep $(( RANDOM % 300 + 1 ))
      - id: generate-cluster-name
        name: generate cluster name
        env:
          SUITE: ${{ inputs.suite }}
          CLUSTER_NAME: ${{ inputs.cluster_name }}
          WORKFLOW_TRIGGER: ${{ inputs.workflow_trigger }}
        run: |
          if [[ "$CLUSTER_NAME" == '' ]]; then
            if [[ "$WORKFLOW_TRIGGER" == 'soak' ]]; then
              CLUSTER_NAME=$(echo "soak-periodic-$RANDOM$RANDOM" | awk '{print tolower($0)}' | tr / -)
            else
              CLUSTER_NAME=$(echo "$SUITE-$RANDOM$RANDOM" | awk '{print tolower($0)}' | tr / -)
            fi
          fi
          echo "Using cluster name \"$CLUSTER_NAME\""
          echo CLUSTER_NAME="$CLUSTER_NAME" >> "$GITHUB_OUTPUT"
      - name: setup eks cluster '${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}'
        if: inputs.cluster_name == ''
        uses: ./.github/actions/e2e/setup-cluster
        with:
          account_id: ${{ vars.CI_ACCOUNT_ID }}
          role: ${{ vars.CI_ROLE_NAME }}
          region: ${{ inputs.region }}
          cluster_name: ${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}
          k8s_version: ${{ inputs.k8s_version }}
          eksctl_version: v0.169.0
          ip_family: ${{ contains(inputs.suite, 'IPv6') && 'IPv6' || 'IPv4' }} # Set the value to IPv6 if IPv6 suite, else IPv4
          private_cluster: ${{ inputs.suite == 'PrivateCluster' }}
          git_ref: ${{ inputs.git_ref }}
          ecr_account_id: ${{ vars.SNAPSHOT_ACCOUNT_ID }}
          ecr_region: ${{ vars.SNAPSHOT_REGION }}
          prometheus_workspace_id: ${{ vars.WORKSPACE_ID }}
          prometheus_region: ${{ vars.PROMETHEUS_REGION }}
          enable_local_zones: ${{ inputs.suite == 'LocalZone' }}
          cleanup: ${{ inputs.cleanup }}
      - name: run tests for private cluster
        if: ${{ inputs.suite == 'PrivateCluster' }}
        env:
          # If we are performing the PrivateCluster test suite, then we should just run the 'Integration' test suite
          SUITE: "Integration"
          CLUSTER_NAME: ${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}
          INTERRUPTION_QUEUE: ${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}
          VERSION: v3.12.3 # Pinned to this version since v3.13.0 has issues with anonymous pulls: https://github.com/helm/helm/issues/12423
          PROMETHEUS_REGION: ${{ vars.PROMETHEUS_REGION }}
          WORKSPACE_ID: ${{ vars.WORKSPACE_ID }}
          ACCOUNT_ID: ${{ vars.CI_ACCOUNT_ID }}
          K8S_VERSION: ${{ inputs.k8s_version }}
          ECR_ACCOUNT_ID: ${{ vars.SNAPSHOT_ACCOUNT_ID }}
          ECR_REGION: ${{ vars.SNAPSHOT_REGION }}
          PRIVATE_CLUSTER: ${{ inputs.suite == 'PrivateCluster' }}
          ENABLE_METRICS: ${{ inputs.enable_metrics }}
          METRICS_REGION: ${{ vars.TIMESTREAM_REGION }}
        uses: aws-actions/aws-codebuild-run-build@bafa4d8b0d8802b5adf3a54861f530792d2e4f24 #v1.0.15
        with:
          project-name: E2EPrivateClusterCodeBuildProject-us-west-2
          buildspec-override: |
            version: 0.2
            phases:
              install:
                commands:
                  # Make sure goenv is up to date
                  - cd $HOME/.goenv && git pull --ff-only && cd -
                  # Install Go 1.22
                  - goenv install 1.22 && goenv global 1.22
              build:
                commands:
                  - aws eks update-kubeconfig --name $CLUSTER_NAME
                  - aws ecr get-login-password --region $ECR_REGION | docker login --username AWS --password-stdin $ECR_ACCOUNT_ID.dkr.ecr.$ECR_REGION.amazonaws.com
                  - chmod +x ./hack/scripts/noderole_bootstrap_permission.sh && ./hack/scripts/noderole_bootstrap_permission.sh
                  - chmod +x ./hack/scripts/install_helm.sh && ./hack/scripts/install_helm.sh
                  - helm plugin install https://github.com/databus23/helm-diff || true
                  - helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
                  - helm pull prometheus-community/kube-prometheus-stack
                  - kubectl create ns prometheus || true
                  - kubectl label ns prometheus scrape=enabled --overwrite=true
                  - chmod +x ./hack/scripts/install_prometheus.sh && ./hack/scripts/install_prometheus.sh
                  - kubectl label ns kube-system scrape=enabled --overwrite=true
                  - kubectl label ns kube-system pod-security.kubernetes.io/warn=restricted --overwrite=true
                  - chmod +x ./hack/scripts/install_karpenter.sh && ./hack/scripts/install_karpenter.sh
                  - chmod +x ./hack/scripts/diff_karpenter.sh && ./hack/scripts/diff_karpenter.sh
                  - kubectl delete nodepool --all
                  - kubectl delete ec2nodeclass --all
                  - kubectl delete deployment --all
                  # Use pull through cache to pull images that are needed for the tests to run as it requires a route to the internet for the first time
                  - docker pull 069919849861.dkr.ecr.us-west-2.amazonaws.com/k8s/pause:3.6
                  - docker pull 069919849861.dkr.ecr.us-west-2.amazonaws.com/ecr-public/eks-distro/kubernetes/pause:3.2
                  - PRIVATE_CLUSTER=$CLUSTER_NAME TEST_SUITE="Integration" ENABLE_METRICS=$ENABLE_METRICS METRICS_REGION=$METRICS_REGION GIT_REF="$(git rev-parse HEAD)" CLUSTER_NAME=$CLUSTER_NAME CLUSTER_ENDPOINT="$(aws eks describe-cluster --name $CLUSTER_NAME --query "cluster.endpoint" --output text)" INTERRUPTION_QUEUE=$CLUSTER_NAME make e2etests
          env-vars-for-codebuild: |
            SUITE,
            CLUSTER_NAME,
            INTERRUPTION_QUEUE,
            VERSION,
            PROMETHEUS_REGION,
            WORKSPACE_ID,
            ACCOUNT_ID,
            K8S_VERSION,
            ECR_ACCOUNT_ID,
            ECR_REGION,
            PRIVATE_CLUSTER,
            ENABLE_METRICS,
            METRICS_REGION
      - name: run the ${{ inputs.suite }} test suite
        if: ${{ inputs.suite != 'PrivateCluster' }}
        env:
          SUITE: ${{ inputs.suite }}
          ENABLE_METRICS: ${{ inputs.enable_metrics }}
        run: |
          aws eks update-kubeconfig --name ${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}
          # Clean up the cluster before running all tests
          kubectl delete nodepool --all
          kubectl delete ec2nodeclass --all
          kubectl delete deployment --all

          TEST_SUITE="$SUITE" ENABLE_METRICS=$ENABLE_METRICS METRICS_REGION=${{ vars.TIMESTREAM_REGION }} GIT_REF="$(git rev-parse HEAD)" \
            CLUSTER_NAME="${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}" CLUSTER_ENDPOINT="$(aws eks describe-cluster --name ${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }} --query "cluster.endpoint" --output text)" \
            INTERRUPTION_QUEUE="${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}" make e2etests
      - name: notify slack of success or failure
        uses: ./.github/actions/e2e/slack/notify
        if: (success() || failure()) && github.event_name != 'workflow_run' && inputs.workflow_trigger != 'versionCompatibility'
        with:
          cluster_name: ${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}
          url: ${{ inputs.workflow_trigger == 'soak' && secrets.SLACK_WEBHOOK_SOAK_URL || secrets.SLACK_WEBHOOK_URL }}
          suite: ${{ inputs.workflow_trigger == 'soak' && 'soak' || inputs.suite }}
          git_ref: ${{ inputs.git_ref }}
      - name: dump logs on failure
        uses: ./.github/actions/e2e/dump-logs
        if: (failure() || cancelled()) && inputs.suite != 'PrivateCluster'
        with:
          account_id: ${{ vars.CI_ACCOUNT_ID }}
          role: ${{ vars.CI_ROLE_NAME }}
          region: ${{ inputs.region }}
          cluster_name: ${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}
      # In the case of failure, check if the managed node group is unhealthy. If so, do not clean up cluster for further investigation.
      # TODO: @jmdeal remove after investigation is complete
      - name: detect unhealthy mng
        id: detect-unhealthy-mng
        shell: bash
        if: (failure() || cancelled()) && inputs.suite != 'PrivateCluster'
        run: |
          if ! kubectl get nodes -l eks.amazonaws.com/nodegroup -oyaml | yq ".items[].status.conditions" | grep -q "KubeletNotReady"; then
            echo UNHEALTHY="false" >> "$GITHUB_OUTPUT"
          else
            echo UNHEALTHY="true" >> "$GITHUB_OUTPUT"
          fi
      - name: cleanup karpenter and cluster '${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}' resources
        uses: ./.github/actions/e2e/cleanup
        if: always() && inputs.cleanup && (steps.detect-unhealthy-mng.conclusion == 'skipped' || steps.detect-unhealthy-mng.outputs.UNHEALTHY == 'false')
        with:
          account_id: ${{ vars.CI_ACCOUNT_ID }}
          role: ${{ vars.CI_ROLE_NAME }}
          region: ${{ inputs.region }}
          cluster_name: ${{ steps.generate-cluster-name.outputs.CLUSTER_NAME }}
          git_ref: ${{ inputs.git_ref }}
          eksctl_version: v0.169.0
          private_cluster: ${{ inputs.suite == 'PrivateCluster' }}
      - if: always() && github.event_name == 'workflow_run'
        uses: ./.github/actions/commit-status/end
        with:
          name: ${{ github.workflow }} (${{ inputs.k8s_version }}) / e2e (${{ inputs.suite }})
          git_ref: ${{ inputs.git_ref }}
